% Chapter 1
\chapter{Background: statistical modeling of calcium imaging data}
\fancyhead[RO,LE]{\thepage}
\fancyhead[LO]{Chapter 1 - \emph{Background}}
\fancyhead[RE]{Section \thesection \ - \emph{\Sectionname}}

\setlength{\parskip}{0.5pt}

\bigskip

\section{Overview of calcium imaging data} 

Calcium ions generate intracellular signals that control key functions in all types of neurons.
At rest, most neurons have an intracellular calcium concentration of about 100 nm; however, during electrical activity, the concentration can rise transiently up to levels around 1000 nm~\citep{berridge2000}. 
The development of techniques that enable the visualization and quantitative estimation of the intracellular calcium signals have thus greatly enhanced the investigation of neuronal functioning.
The development of calcium imaging techniques involved two parallel processes: the development of calcium indicators, which are fluorescent molecules that react when binding to the calcium ions, and the implementation of the appropriate imaging instrumentation, in particular, the introduction of two-photon microscopy~\citep{denk1990}.
In recent years, the innovation achieved in these two fields has allowed for real-time observation of biological processes at the single-cell level simultaneously for large groups of neurons~\citep{grienberger2012}. 

The output two-photon calcium imaging is a movie of time-varying fluorescence intensities, and a first complex pre-processing phase deals with the identification of the spatial location of each neuron in the optical field and source extraction~\citep{mukamel2009,dombeck2010}. The resulting processed data consist of a fluorescent calcium trace for each observable neuron in the targeted area which, however, is only a proxy of the underlying neuronal activity.
Hence further analyses are needed to deconvolve the fluorescence trace to extract the spike train (i.e. the series of recorded firing times), and to try to explain how these firing events are linked with the experiment that generated that particular pattern of activity.

\subsection{Deconvolution methods}

There is currently a rich literature of methods addressing the issue of deconvolving the raw fluorescent trace to extract the spike train. A successful approach is to assume a biophysical model to relate the spiking activity to the calcium dynamics, and to the observed fluorescence. \citet{vogelstein2010} proposed a simple but effective model that has later been adopted by several authors~\citep{pnevmatikakis2016, friedrich2016, friedrich2017, jewell2018, jewell2019}. The model considers the observed fluorescence as a linear (and noisy) function of the intracellular calcium concentration; the calcium dynamics is then modeled using an autoregressive process with jumps in correspondence of the neuron's firing events.
Denoting with $y_t$ the observed fluorescence trace of a neuron and with $c_t$ the underlying calcium concentration, for time $t=1,\dots,T$, the model can be written as
\begin{equation}
\begin{gathered}
y_t = b + c_t + \epsilon_t,\quad \epsilon_t \sim \N(0,\sigma^2),  \\
c_t = \gamma\, c_{t-1} + A_t + w_t, \quad w_t \sim \N(0, \tau^2),
\end{gathered}
\label{eq:ch1_armodel}
\end{equation}
where $b$ models the baseline level of the observed trace and $\epsilon_t$ is a Gaussian measurement error. In the absence of neuronal activity, the true calcium concentration $c_t$ is considered to be centered around zero. The parameter $A_t$ captures the neuronal activity: in the absence of a spike ($A_t = 0$), the calcium level follows a AR(1) process controlled by the parameter $\gamma$; when a spike occurs, the concentration increases instantaneously of a value $A_t > 0$.
A challenge remains estimating the neuronal activity $A_t$ in a precise and computationally efficient way.

\citet{vogelstein2010} assume that all spikes have a fixed amplitude, and interpret the parameter $A_t$ as the \textit{number} of spikes at time $t$. Following this definition, they place a Poisson prior distribution on $A_t$; however, the maximum a posteriori estimation of the spike train using a Poisson distribution is computationally intractable. Hence they search an approximate solution by replacing the Poisson distribution with an exponential distribution of the same mean. This leads to some loss of interpretation of the parameters $A_t$, as now they are no longer integer values but rather non-negative real numbers, but turns the problem into a convex optimization, which can be solved efficiently.
Adopting this approach leads to solving a non-negative lasso problem for estimating the calcium concentration, where the $L_1$ penalty enforces sparsity of the neural activity.
Efficient algorithms to obtain a solution of this problem were also proposed by \citet{pnevmatikakis2016}, \citet{friedrich2016}, and \citet{friedrich2017}.

A different perspective is instead proposed by~\citet{jewell2018} and~\citet{jewell2019}: rather than interpreting $A_t$ in model~(\ref{eq:ch1_armodel}) as the number of spikes at the $t$-th timestep, they interpret its sign as an indicator for whether or not \textit{at least one} spike occurred, that is, $A_t = 0$ indicates no spikes at time $t$, and $A_t>0$ indicates the occurrence of at least one spike. The model so formulated includes an indicator variable, which corresponds to using an $L_0$ penalization and which makes the optimization problem highly non-convex. 
In their work, \citet{jewell2018} and~\citet{jewell2019} develop fast algorithms to compute the spike trains under these assumptions.
\citet{jewell2018} assert that the solutions discussed by \citet{vogelstein2010}, \citet{friedrich2016}, and \citet{friedrich2017} can actually be seen as convex relaxations of this optimization problem, to overcome the computational intractability of the $L_0$ penalization. 

Finally,~\citet{pnevmatikakis2013} propose a fully Bayesian approach. Although less computationally efficient than optimization methods, it allows to obtain a posterior distribution of all model parameters instead of just a point estimate, hence improving uncertainty quantification.
Differently from previous models, they define the parameter $A_t$ as the \textit{amplitude} of a spike at time $t$, taking values in the non-negative real numbers.
They formulate the presence/absence of a spike and its amplitude by using the product of a Bernoulli random variable (taking value 0 if there is no spike at time $t$, and 1 otherwise) with a half-Gaussian random variable (modeling the positive amplitudes). However, they do not explicitly assume sparsity of the spikes.

\section{Data sets} 
Una frase introduttiva? Parlo dei dati dell'Allen Brain Observatory e poi ci sarebbe da mettere i nuovi dati se riesco a fare qualcosa del progetto 3...

\subsection{Allen Brain Observatory data}
The Allen Brain Observatory~\citep{allen} is a public large data repository for investigating how sensory stimulation is represented by neural activity in the mouse visual cortex in both single cells and populations.
The project aims to provide a standardized and systematic survey to measure and analyze visual responses from neurons across cortical areas and layers, utilizing transgenic Cre lines to drive expression of genetically encoded fluorescent calcium indicators, and measured by \textit{in vivo} two-photon calcium imaging.

The study is an extended survey of physiological activity in the mouse visual cortex in response to a range of visual stimuli~\citep{allen_stimulus}. Each mouse is placed in front of a screen where different types of visual stimuli are shown, while the mouseâ€™s neuronal activity is recorded. The stimuli vary from simple synthetic images such as locally sparse noise or static gratings, to complex natural scenes and movies.
The goal of the study is to investigate how neurons at different depths in the visual areas respond to stimuli of different complexity. Specifically, each neuron in the visual cortex can be characterized by their \textit{receptive field}, i.e. the features of the visual stimulus that trigger the signaling of that neuron. Hence, it is of critical interest to devise methods that allow inferring how the neuronal response varies under the different types of visual stimuli. %We expect that the neuronal activity will vary across all the experimental settings, and that some variations in its intensity will be observed based on the specific visual stimulus.


\subsection{Altri dati?}
Paragrafo qui.

\section{A brief review of some Bayesian nonparametric models} 
In this section we review some statistical tools that will be employed in this thesis in the analysis of calcium imaging data. The purpose of this section is not to provide a comprehensive review, but rather to outline the theoretical framework we adopted and fix some notation.
The core topic will be the Bayesian methodology, with a focus on Bayesian nonparametric models.

\subsection{Finite mixture models}
\label{subsec:finite_mix}
We start our discussion by reviewing finite mixtures. Although they are not part of the Bayesian nonparametric methodology, they provide the starting point for many models that we will review in the following. The content of this brief overview on finite mixtures is largely based on the dedicated chapter in~\citet{gelman2013}.

\subsubsection*{Definition and hierarchical representations}
Mixtures are a popular tool to model heterogeneous data, characterized by the presence of subpopulations within the overall population. In many practical problems the data are collected under different conditions -- unfortunately, it is not always possible to have information on the subpopulation to which each individual observation belongs.
Mixture models can be used in problems of this type, where the population consists of a number of latent subpopulations, within each of which a relatively simple model can be applied.

Denote the observed data as a vector of $n$ units $\bm{y} = (y_1,\dots,y_n)$; also, assume that the $n$ observations are exchangeable, meaning that the joint probability density $p(y_1,\dots,y_n)$ is invariant to permutations of the indices. In the framework of finite mixtures, we assume that the population is made of $K\leq n$ subpopulations, with $K$ known and fixed.
We assume that within each of these groups, the distribution of $y_i$, $i=1,\dots,n$, can be modeled as $f(y_i \mid \theta_k^*)$, for $k=1,\dots,K$. Usually a common parametric family is assumed for all these component distributions, which however depend on specific parameter vectors $\theta_k^*$.
The last missing piece to construct a mixture model is the parameter describing the proportion of population from each component $k$: we denote this parameter with $\pi_k$, satisfying $\sum_{k=1}^K \pi_k = 1$. Denoting the full vectors of parameters as $\bm{\theta}^* = (\theta_1^*,\dots,\theta_K^*)$ and $\bm{\pi} = (\pi_1,\dots,\pi_K)$, the data distribution for observation $i$ can be formulated as
\begin{equation*}
p(y_i\mid \bm{\theta}^*,\bm{\pi}) = \pi_1 \, f(y_i\mid\theta_1^*) + \dots + \pi_K \, f(y_i\mid\theta_K^*).
\end{equation*}

In mixture models it is convenient to think of the component indicators as missing data, and to impute them to obtain a much simpler form of the data distribution. Hence we introduce the indicator $S_{ik}$ of component $k$ for observation $i$, with 
\begin{equation*}
S_{ik} = \begin{cases}
1 \quad \text{if $y_i$ is drawn from component $k$}\\
0 \quad \text{otherwise}.
\end{cases}
\end{equation*}
Given $\bm{\pi}$, the distribution of $\bm{S}_i = (S_{i1},\dots,S_{iK})$ is $\Mult(1;\pi_1,\dots,\pi_K)$. 
Conditionally on $\bm{S}_{i}$, the data distribution of $y_i$ is simply $p(y_i\mid \bm{S}_i,\bm{\theta}^*) = \prod_{k=1}^K f(y_i\mid\theta_k^*)^{S_{ik}}$; moreover, given $\bm{S}=(\bm{S}_1,\dots,\bm{S}_n)$, the $y_i$ are assumed to be independent.
The joint density of the observed data and the unobserved indicators, conditionally on the model parameters, can now be written as 
\begin{equation*}
p(\bm{y},\bm{S}\mid \bm{\theta}^*,\bm{\pi}) = p(\bm{y}\mid\bm{S},\bm{\theta}^*)\, p(\bm{S}\mid\bm{\pi}) = 
\prod_{i=1}^n \prod_{k=1}^K \left\{ \pi_k f(y_i\mid\theta_k^*) \right\}^{S_{ik}}.
\end{equation*}

Having defined the data distribution, we need to specify adequate prior distributions on the model parameters $\bm{\pi}$ and $\bm{\theta}^*$. The prior $G_0$ on $\bm{\theta}^*$ is usually chosen depending on the specific application and on the basis of the component distribution $f$. For the mixture proportions $\pi_k$, the conjugate and most natural prior distribution is the Dirichlet distribution, $\bm{\pi}\sim \Dir_K(\alpha_1,\dots,\alpha_K)$.

This model also admits a useful hierarchical representation. Rewriting the latent allocation variables using the cluster indicators $c_i\in\{1,\dots,K\}$, with $c_i=k$ if $y_i$ belongs to the $k$-th mixture component (i.e. $S_{ik} = 1$), the model is, for $i = 1,\dots,n$ 
\begin{equation}
\begin{gathered}
\pi_1,\dots,\pi_K  \sim \Dir_K(\alpha_1,\dots,\alpha_K)\\
\theta^*_1,\dots,\theta^*_K \sim G_0\\
\Pr(c_i = k \mid \pi_1,\dots,\pi_K) = \pi_k \quad \text{for } k = 1,\dots,K\\ 
y_i \mid c_i = k, \theta^*_k \sim f(y_i\mid\theta^*_k).
\end{gathered}
\label{eq:ch1_finitemix_h1}
\end{equation}

It is possible to rewrite equation (\ref{eq:ch1_finitemix_h1}) in a slightly different way by thinking that each observation $y_i$ is associated with a parameter $\theta_i$, where these parameters are drawn from a discrete distribution $G$ with support on the $K$ locations $\{\theta^*_1,\dots,\theta^*_K\}$. The model then becomes, for $i=1,\dots,n$
\begin{equation}
\begin{gathered}
\pi_1,\dots,\pi_K \sim \Dir_K(\alpha_1,\dots,\alpha_K) \\
\theta^*_1,\dots,\theta^*_K \sim G_0\\
\theta_i \mid \bm{\theta}^*,\bm{\pi} \sim G = \sum_{k=1}^K \pi_k \delta_{\theta^*_k}\\
y_i \mid \theta_i \sim f(y_i\mid\theta_i).
\end{gathered}
\label{eq:ch1_finitemix_h2}
\end{equation}

\subsubsection*{Posterior inference for finite mixture models}
Posterior inference for mixture models is usually performed through Markov Chain Monte Carlo (MCMC) methods and, in particular, the Gibbs sampler, as the full conditionals after imputing the cluster indicators $\CC = \{c_1,\dots,c_n\}$ are greatly simplified.
Moreover, for the distribution of the mixture weights it is possible to exploit the conjugacy of the Dirichlet distribution with the multinomial model. A Gibbs sampler then simply iterates these three steps:
\begin{enumerate}
	\item Update the cluster-specific parameters $\theta^*_k$, for $k=1,\dots,K$, from
	$$ p(\theta^*_k \mid \CC, \bm{y}) \propto G_0(\theta^*_k) \prod_{i:c_i=k} f(y_i\mid\theta^*_{k}). $$
	\item Update the weights $\pi_1,\dots,\pi_K$ by sampling from a Dirichlet distribution with updated parameters
	$$ \pi_1,\dots,\pi_K\mid \CC \sim  \Dir_K(\alpha_1 + n_1,\dots,\alpha_K + n_K) $$
	where $n_k$ is the number of observations allocated to cluster $k$, for $k=1,\dots,K$.
	\item Update the cluster indicators: for $i=1,\dots,n$ and $k=1,\dots,K$,
	$$ \Pr(c_i = k\mid \bm{\pi},\bm{\theta}^*, y_i) \propto \pi_k f(y_i\mid\theta^*_{k})  .$$
\end{enumerate}

\subsection{Dirichlet process mixtures}
Nonparametric mixtures extend model (\ref{eq:ch1_finitemix_h2}) by placing a nonparametric prior on $G$. The most common prior on random probability measures is the Dirichlet process (DP), introduced by \citet{ferguson1973, ferguson1974}. Draws from a DP are discrete distributions with probability one, hence they turned out useful as flexible mixing measures in discrete mixtures.

\subsubsection*{The Dirichlet process }
A random distribution $G$ on $\Theta$ is said to follow a DP prior with base measure $G_0$ and concentration parameter $\alpha$, denoted $G\sim\DP(\alpha,G_0)$, if for any partition $\{B_1,\dots,B_H\}$ of $\Theta$, 
\begin{equation*}
(G(B_1),\dots,G(B_H)) \sim \Dir_H\big(\alpha G_0(B_1),\dots,\alpha G_0(B_H)\big).
\end{equation*}
The success of the DP mainly arises from two appealing characteristics: its large support, with respect to the space of probability distributions, and tractability of the posterior distribution.
%
Closely related to this last aspect is the conjugacy property of the DP: as the finite dimensional Dirichlet distribution is conjugate to the multinomial likelihood, the DP is conjugate with respect to i.i.d. sampling, that is, with respect to a completely unknown distribution from i.i.d. data.
More precisely, if we take $\{\theta_1,\dots,\theta_n\}$ a sequence of independent draws from $G\sim\DP(\alpha,G_0)$, then the posterior distribution of $G$ given these observed values is still a DP. 
Letting again $\{B_1,\dots,B_H\}$ be a finite measurable partition of $\Theta$, and letting $n_h$ be the number of observed values in $B_h$, for $h = 1,\dots,H$, the posterior distribution is given by
\begin{equation*}
(G(B_1),\dots,G(B_H))\mid \theta_1,\dots,\theta_n \sim \Dir_H\big(\alpha G_0(B_1) + n_1,\dots,\alpha G_0(B_H) + n_H\big).
\end{equation*}
In other terms, the posterior distribution is still DP with updated parameters:
\begin{equation*}
G \mid \theta_1,\dots,\theta_n \sim \DP \left(\alpha + n, \frac{\alpha G_0 + \sum_{i=1}^n \delta_{\theta_i}}{\alpha + n} \right).
\end{equation*}

The DP admits several nice representations. An intuitive constructive definition of a DP random probability measure is given by \citet{sethuraman1994} and is based on the discrete nature of the process, which can be represented as a a weighted sum of point masses. This definition states that if  $G\sim \DP(\alpha, G_0)$, then it can be expressed as follows:
\begin{equation*}
\begin{gathered}
\beta_k \sim \Beta(1,\alpha), \qquad \pi_k = \beta_k \prod_{l = 1}^{k-1} (1-\beta_l)\\
\theta^*_k \sim G_0, \qquad
G = \sum_{k=1}^{\infty} \pi_k \delta_{\theta^*_k}.
\end{gathered}
\end{equation*}

\subsubsection*{Dirichlet process mixtures}
Getting back to the framework of mixture models, from this representation it is straightforward to extend the finite mixtures described above to nonparametric mixtures: following the structure of equation (\ref{eq:ch1_finitemix_h2}), DP mixtures (DPM) can be written as
\begin{equation*}
\begin{gathered}
G \sim \DP(\alpha,G_0)\\
\theta_i \mid G \sim G \\
y_i \mid \theta_i \sim f(y_i\mid\theta_i).
\end{gathered}
\end{equation*}
Differently from the mixtures described in the previous section, DPM are an infinite mixture model, as they assume a countably infinite number of components. 
However, because the $\pi_k$'s decrease exponentially quickly, only a small number of components will be used to model the
data a priori: in the following, we will define \textit{clusters} these non-empty components. In general, the prior expected number of clusters, which corresponds to the number of distinct $\theta_i$ in a sample of $n$ values, is approximately equal to $\alpha \log(1 + n/\alpha)$.
This means that while in finite mixture models the number of clusters has to be fixed in advance, in DP mixture model, the number of clusters is determined by the data and can be inferred.

\subsubsection*{Posterior inference for DPM}


\subsection{Mixtures of finite mixtures}
To avoid fixing the number of clusters, a different approach to DP mixtures is to consider finite mixtures with a prior on the number of components. Although this may seem as the most natural way to infer the number of groups, application of mixtures of finite mixtures (MFM) has long been hindered by the difficulty of performing posterior inference. 
Several inference methods have been proposed for this type of model \citep{McCullagh2008,nobile2004,nobile2007,richardson1997} often exploiting the reversible jump Markov chain Monte Carlo technique. However, applying the reversible jump in new situations can be hard, as it requires designing new reversible jump moves.

More recently, different models have been proposed inspired by nonparametric mixtures. \citet{miller2018} explicitly derived MFM counterparts for several properties exhibited by DPM. They considered a finite mixture model similar to the one defined in section \ref{subsec:finite_mix}, where however the number of clusters is random and is assigned a prior distribution. A main limitation of their approach is that it is assumed a fixed parameter $\alpha$ for the Dirichlet distribution, regardless of the dimension $K$.

Another approach is discussed by \citet{malsinerwalli2016} and \cite{fs2019}, based on the use of sparse mixtures. Similarly to DPM, in this formulation they distinguish between the mixture components and the clusters, which are defined as the components actually used by the data. In their approach, the number of components $K$ is fixed to a large and clearly overfitting value, and a Dirichlet prior with small parameter on the mixture weights ensures that the (random) number of clusters $K_+$ will take values smaller than $K$ with high probability a priori and also a posteriori.

\subsubsection*{Generalized mixtures of finite mixtures}
A general formulation that includes all three models described above as special cases (finite mixtures with a fixed number of clusters, MFM and overfitted mixtures) has recently been described by \citet{fruhwirthschnatter2020}: they call this specification generalized mixtures of finite mixtures (gMFM). Similarly to overfitted mixtures, a key aspect of this approach is the distinction between the number of components $K$ and the number of clusters $K_+$; however, here, the number of components is also random. 
In the following we will review some aspects of these models that will be used later in this thesis.
The gMFM model can be defined in a hierarchical form analogous to equation (\ref{eq:ch1_finitemix_h1}) as
\begin{equation*}
\begin{gathered}
K \sim p(K)\\
\pi_1,\dots,\pi_K \mid K, \alpha \sim \Dir_K(\alpha / K, \dots, \alpha / K)\\
\theta^*_1,\dots,\theta^*_K \sim G_0\\
\Pr(c_i = k \mid K, \pi_1,\dots,\pi_K) = \pi_k \quad \text{for } k = 1,\dots,K\\ 
y_i \mid K, c_i = k, \theta^*_k \sim f(y_i\mid\theta^*_k).
\end{gathered}
\label{eq:ch1_gMFM}
\end{equation*}
Including a prior on $K$ also induces a prior on the number of clusters $K_+$: here a crucial role is played by the sequence of concentration parameters of the Dirichlet distribution. Considering fixed parameters as in~\citet{miller2018}, where a $\Dir_K(1,\dots,1)$ is used regardless of the value of $K$, leads to a prior expected number of clusters $E(K_+)$ close to $E(K)$ for many priors $p(K)$. Having instead concentration parameters that decrease with increasing $K$ induces randomness in the prior distribution of $K_+\mid K$, allowing for a gap between $K_+$ and $K$. In this formulation the parameters decrease linearly with $K$, and a $F$ prior distribution is used for the hyperparameter $\alpha$.
The specification of gMFM is completed with a suitable prior $p(K)$ on the number of components. In their work, \citet{fruhwirthschnatter2020} discuss different choices and compare the resulting prior on the number of clusters. 
A desirable prior on $K$ should be weakly informative on the number of clusters, and should lead to a prior on $K_+$ which is concentrated on moderate number of clusters, with fat tails to ensure that also a high number of clusters may be estimated. They suggest to use a translated prior for $K$, where $K-1$ follows a beta-negative-binomial (BNB) distribution, which is a hierarchical generalization of the Poisson, the geometric and the negative-binomial distribution.

\subsubsection*{Posterior inference for gMFM}
An important contribution of the work of~\citet{fruhwirthschnatter2020} is the introduction of a new inference algorithm ``telescoping sampler'' to obtain the posterior distribution of all model parameters without resorting to reversible jump MCMC techniques. Their algorithm is a trans-dimensional Gibbs sampler which at each iteration alternates two key steps: first, it updates the partition of the observations $\CC= \{c_1,\dots,c_n\}$ conditionally on the number of components, then, it samples a new value for $K$ on the basis of this partition.
Explicitly sampling the number of mixture components greatly simplifies inference as, conditionally on $K$, the updates of the partition and of the model parameters are brought back to standard steps.
%
Hence the crucial aspect is sampling from the full conditional of the number of components. This is achieved by combining the conditional exchangeable partition probability function $p(\CC \mid n, K, \alpha)$, derived in Section 2.2 of~\citet{fruhwirthschnatter2020}, with the prior $p(K)$.
%
The algorithm performs the following steps:
\begin{enumerate}
	\item Update the partition $\CC$:
	\begin{enumerate}
		\item[(a)] Sample $c_i$, for $i=1,\dots,n$ from $\Pr(c_i = k \mid \bm{\pi}, \bm{\theta}^*, y)\propto \pi_k f(y_i\mid\theta^*_k)$;
		\item[(b)] Determine the number $K_+$ of non-empty clusters and relabel the components such that the first $K_+$ clusters are non-empty.
	\end{enumerate}
	\item Conditional on $\CC$, update the parameters of the non-empty components (and eventual hyperparamters): $$p(\theta^*_k\mid \CC,\bm{y}) \propto G_0(\bm{\theta}^*) \prod_{i:c_i=k} f(y_i\mid\theta^*_k)\quad \text{for } k = 1,\dots,K.$$
	\item Conditional on $\CC$, draw a new value for $K$ from
	\begin{equation*}
	p(K\mid \CC, \alpha) \propto p(\CC\mid n, K, \alpha) p(K) = p(K) \frac{K!\,\alpha^{K_+}}{(K-K_+)!\,K^{K_+}} \prod_{k=1}^{K_+} \frac{\Gamma(n_k + \alpha/K)}{\Gamma(1+\alpha/K)}
	\end{equation*}
	for $K = K_+, K_++1,\dots$; where $n_k$ is the number of observations in cluster $k$.
	\item Update $\alpha$ by performing a Metropolis-Hastings step to sample $\alpha$ from its full conditional 
	$$ p(\alpha\mid\CC,K) \propto p(\alpha) \frac{\alpha^{K_+}\Gamma(\alpha)}{\Gamma(n+\alpha)} \prod_{k=1}^{K_+} \frac{\Gamma(n_k + \alpha/K)}{\Gamma(1+\alpha/K)} $$
	\item Add $K-K_+$ empty components,
	\begin{itemize}
		\item[(a)] if $K>K_+$, sample $K-K_+$ new values $\theta^*_k$ from the prior, $k = K_+ + 1,\dots,K$;
		\item[(b)] update the weight vector as $$\pi_1,\dots,\pi_K \mid K, \alpha, \CC \sim \Dir_K(\alpha/K + n_1, \dots, \alpha/K + n_K).$$ 
	\end{itemize}
\end{enumerate}




\subsection{Bayesian nonparametric models for nested data}
All models described so far assumed that the data were exchangeable, that is, they arise from one unknown distribution. However, there is growing interest in modeling scenarios where the data come from different but related groups, as for example different populations or experiments. In these cases it is desirable to individually model the distribution of each group, while also borrowing information between them.
















% Chapter 1
\chapter{Background: statistical modeling of calcium imaging data}
\fancyhead[RO,LE]{\thepage}
\fancyhead[LO]{Chapter 1 - \emph{Background}}
\fancyhead[RE]{Section \thesection \ - \emph{\Sectionname}}

\setlength{\parskip}{0.5pt}

\bigskip

\section{Overview of calcium imaging data} 

Calcium ions generate intracellular signals that control key functions in all types of neurons.
At rest, most neurons have an intracellular calcium concentration of about 100 nm; however, during electrical activity, the concentration can rise transiently up to levels around 1000 nm~\citep{berridge2000}. 
The development of techniques that enable the visualization and quantitative estimation of the intracellular calcium signals have thus greatly enhanced the investigation of neuronal functioning.
The development of calcium imaging techniques involved two parallel processes: the development of calcium indicators, which are fluorescent molecules that react when binding to the calcium ions, and the implementation of the appropriate imaging instrumentation, in particular, the introduction of two-photon microscopy~\citep{denk1990}.
In recent years, the innovation achieved in these two fields has allowed for real-time observation of biological processes at the single-cell level simultaneously for large groups of neurons~\citep{grienberger2012}. 

The output two-photon calcium imaging is a movie of time-varying fluorescence intensities, and a first complex pre-processing phase deals with the identification of the spatial location of each neuron in the optical field and source extraction~\citep{mukamel2009,dombeck2010}. The resulting processed data consist of a fluorescent calcium trace for each observable neuron in the targeted area which, however, is only a proxy of the underlying neuronal activity.
Hence further analyses are needed to deconvolve the fluorescence trace to extract the spike train (i.e. the series of recorded firing times), and to try to explain how these firing events are linked with the experiment that generated that particular pattern of activity.

\subsection{Deconvolution methods}

There is currently a rich literature of methods addressing the issue of deconvolving the raw fluorescent trace to extract the spike train. A successful approach is to assume a biophysical model to relate the spiking activity to the calcium dynamics, and to the observed fluorescence. \citet{vogelstein2010} proposed a simple but effective model that has later been adopted by several authors~\citep{pnevmatikakis2016, friedrich2016, friedrich2017, jewell2018, jewell2019}. The model considers the observed fluorescence as a linear (and noisy) function of the intracellular calcium concentration; the calcium dynamics is then modeled using an autoregressive process with jumps in correspondence of the neuron's firing events.
Denoting with $y_t$ the observed fluorescence trace of a neuron and with $c_t$ the underlying calcium concentration, for time $t=1,\dots,T$, the model can be written as
\begin{equation}
\begin{gathered}
y_t = b + c_t + \epsilon_t,\quad \epsilon_t \sim \mathrm{N}(0,\sigma^2),  \\
c_t = \gamma\, c_{t-1} + A_t + w_t, \quad w_t \sim \mathrm{N}(0, \tau^2),
\end{gathered}
\label{eq:ch1_armodel}
\end{equation}
where $b$ models the baseline level of the observed trace and $\epsilon_t$ is a Gaussian measurement error. In the absence of neuronal activity, the true calcium concentration $c_t$ is considered to be centered around zero. The parameter $A_t$ captures the neuronal activity: in the absence of a spike ($A_t = 0$), the calcium level follows a AR(1) process controlled by the parameter $\gamma$; when a spike occurs, the concentration increases instantaneously of a value $A_t > 0$.
A challenge remains estimating the neuronal activity $A_t$ in a precise and computationally efficient way.

\citet{vogelstein2010} assume that all spikes have a fixed amplitude, and interpret the parameter $A_t$ as the \textit{number} of spikes at time $t$. Following this definition, they place a Poisson prior distribution on $A_t$; however, the maximum a posteriori estimation of the spike train using a Poisson distribution is computationally intractable. Hence they search an approximate solution by replacing the Poisson distribution with an exponential distribution of the same mean. This leads to some loss of interpretation of the parameters $A_t$, as now they are no longer integer values but rather non-negative real numbers, but turns the problem into a convex optimization, which can be solved efficiently.
Adopting this approach leads to solving a non-negative lasso problem for estimating the calcium concentration, where the $L_1$ penalty enforces sparsity of the neural activity.
Efficient algorithms to obtain a solution of this problem were also proposed by \citet{pnevmatikakis2016}, \citet{friedrich2016}, and \citet{friedrich2017}.

A different perspective is instead proposed by~\citet{jewell2018} and~\citet{jewell2019}: rather than interpreting $A_t$ in model~(\ref{eq:ch1_armodel}) as the number of spikes at the $t$-th timestep, they interpret its sign as an indicator for whether or not \textit{at least one} spike occurred, that is, $A_t = 0$ indicates no spikes at time $t$, and $A_t>0$ indicates the occurrence of at least one spike. The model so formulated includes an indicator variable, which corresponds to using an $L_0$ penalization and which makes the optimization problem highly non-convex. 
In their work, \citet{jewell2018} and~\citet{jewell2019} develop fast algorithms to compute the spike trains under these assumptions.
\citet{jewell2018} assert that the solutions discussed by \citet{vogelstein2010}, \citet{friedrich2016}, and \citet{friedrich2017} can actually be seen as convex relaxations of this optimization problem, to overcome the computational intractability of the $L_0$ penalization. 

Finally,~\citet{pnevmatikakis2013} propose a fully Bayesian approach. Although less computationally efficient than optimization methods, it allows to obtain a posterior distribution of all model parameters instead of just a point estimate, hence improving uncertainty quantification.
Differently from previous models, they define the parameter $A_t$ as the \textit{amplitude} of a spike at time $t$, taking values in the non-negative real numbers.
They formulate the presence/absence of a spike and its amplitude by using the product of a Bernoulli random variable (taking value 0 if there is no spike at time $t$, and 1 otherwise) with a half-Gaussian random variable (modeling the positive amplitudes). However, they do not explicitly assume sparsity of the spikes.

\section{Data sets} 
Una frase introduttiva? Parlo dei dati dell'Allen Brain Observatory e poi ci sarebbe da mettere i nuovi dati se riesco a fare qualcosa del progetto 3...

\subsection{Allen Brain Observatory data}
The Allen Brain Observatory~\citep{allen} is a public large data repository for investigating how sensory stimulation is represented by neural activity in the mouse visual cortex in both single cells and populations.
The project aims to provide a standardized and systematic survey to measure and analyze visual responses from neurons across cortical areas and layers, utilizing transgenic Cre lines to drive expression of genetically encoded fluorescent calcium indicators, and measured by \textit{in vivo} two-photon calcium imaging.

The study is an extended survey of physiological activity in the mouse visual cortex in response to a range of visual stimuli~\citep{allen_stimulus}. Each mouse is placed in front of a screen where different types of visual stimuli are shown, while the mouseâ€™s neuronal activity is recorded. The stimuli vary from simple synthetic images such as locally sparse noise or static gratings, to complex natural scenes and movies.
The goal of the study is to investigate how neurons at different depths in the visual areas respond to stimuli of different complexity. Specifically, each neuron in the visual cortex can be characterized by their \textit{receptive field}, i.e. the features of the visual stimulus that trigger the signaling of that neuron. Hence, it is of critical interest to devise methods that allow inferring how the neuronal response varies under the different types of visual stimuli. %We expect that the neuronal activity will vary across all the experimental settings, and that some variations in its intensity will be observed based on the specific visual stimulus.


\subsection{Altri dati?}
Paragrafo qui.

\section{A brief review of some Bayesian nonparametric models} 
In this section we review some statistical tools that will be employed in this thesis in the analysis of calcium imaging data. The purpose of this section is not to provide a comprehensive review, but rather to outline the theoretical framework we adopted and fix some notation.
The core topic will be the Bayesian methodology, with a focus on Bayesian nonparametric models.

\subsection{Finite mixture models}
We start our discussion by reviewing finite mixtures. Although they are not part of the Bayesian nonparametric methodology, they provide the starting point for many models that we will review in the following. The content of this brief overview on finite mixtures is largely based on the dedicated chapter in~\citet{gelman2013}.

Mixtures are a popular tool to model heterogeneous data, characterized by the presence of subpopulations within the overall population. In many practical problems the data are collected under different conditions -- unfortunately, it is not always possible to have information on the subpopulation to which each individual observation belongs.
Mixture models can be used in problems of this type, where the population consists of a number of latent subpopulations, within each of which a relatively simple model can be applied.

Denote the observed data as a vector of $n$ units $\bm{y} = (y_1,\dots,y_n)$; also, assume that the $n$ observations are exchangeable, meaning that the joint probability density $p(y_1,\dots,y_n)$ is invariant to permutations of the indices. In the framework of finite mixtures, we assume that the population is made of $K\leq n$ subpopulations, with $K$ known and fixed.
We assume that within each of these groups, the distribution of $y_i$, $i=1,\dots,n$, can be modeled as $f(y_i \mid \theta_k^*)$, for $k=1,\dots,K$. Usually a common parametric family is assumed for all these component distributions, which however depend on specific parameter vectors $\theta_k^*$.
The last missing piece to construct a mixture model is the parameter describing the proportion of population from each component $k$: we denote this parameter with $\pi_k$, satisfying $\sum_{k=1}^K \pi_k = 1$. Denoting the full vectors of parameters as $\bm{\theta}^* = (\theta_1^*,\dots,\theta_K^*)$ and $\bm{\pi} = (\pi_1,\dots,\pi_K)$, the data distribution for observation $i$ can be formulated as
\begin{equation}
p(y_i\mid \bm{\theta}^*,\bm{\pi}) = \pi_1 \, f(y_i\mid\theta_1^*) + \dots + \pi_K \, f(y_i\mid\theta_K^*).
\label{eq:ch1_finitemix}
\end{equation}

In mixture models it is convenient to think of the component indicators as missing data, and to impute them to obtain a much simpler form of the data distribution. Hence we introduce the indicator $S_{ik}$ of component $k$ for observation $i$, with 
\begin{equation*}
S_{ik} = \begin{cases}
1 \quad \text{if $y_i$ is drawn from component $k$}\\
0 \quad \text{otherwise}.
\end{cases}
\end{equation*}
Given $\bm{\pi}$, the distribution of $\bm{S}_i = (S_{i1},\dots,S_{iK})$ is $\mathrm{Multinomial}(1;\pi_1,\dots,\pi_K)$. 
Conditionally on $\bm{S}_{i}$, the data distribution of $y_i$ is simply $p(y_i\mid \bm{S}_i,\bm{\theta}^*) = \prod_{k=1}^K f(y_i\mid\theta_k^*)^{S_{ik}}$; moreover, given $\bm{S}=(\bm{S}_1,\dots,\bm{S}_n)$, the $y_i$ are assumed to be independent.
The joint density of the observed data and the unobserved indicators, conditionally on the model parameters, can now be written as 
\begin{equation*}
p(\bm{y},\bm{S}\mid \bm{\theta}^*,\bm{\pi}) = p(\bm{y}\mid\bm{S},\bm{\theta}^*)\, p(\bm{S}\mid\bm{\pi}) = 
\prod_{i=1}^n \prod_{k=1}^K \left\{ \pi_k f(y_i\mid\theta_k^*) \right\}^{S_{ik}}.
\end{equation*}

Having defined the data distribution, we need to specify adequate prior distributions on the model parameters $\bm{\pi}$ and $\bm{\theta}^*$. The prior $G_0$ on $\bm{\theta}^*$ is usually chosen depending on the specific application and on the basis of the component distribution $f$. For the mixture proportions $\pi_k$, the conjugate and most natural prior distribution is the Dirichlet distribution, $\bm{\pi}\sim \mathrm{Dir}_K(\alpha_1,\dots,\alpha_K)$.

It is possible to rewrite equation (\ref{eq:ch1_finitemix}) in a hierarchical way by thinking that each observation $y_i$ is associated with a parameter $\theta_i$, where these parameters are drawn from a discrete distribution $G$ with support on the $K$ locations $\{\theta^*_1,\dots,\theta^*_K\}$. The model then becomes, for $i=1,\dots,n$
\begin{equation}
\begin{gathered}
y_i \mid \theta_i \sim f(y_i\mid\theta_i)\\
\theta_i \mid \bm{\theta}^*,\bm{\pi} \sim G = \sum_{k=1}^K \pi_k \delta_{\theta^*_k}\\
(\pi_1,\dots,\pi_K) \sim \mathrm{Dir}_K(\alpha_1,\dots,\alpha_K) \\
(\theta^*_1,\dots,\theta^*_K) \sim G_0.
\end{gathered}
\label{eq:ch1_discretemix_h}
\end{equation}

%Posterior inference for mixture models is usually performed through MCMC methods and, in particular, the Gibbs sampler, as the full conditionals after imputing the missing allocation variables are greatly simplified. If we introduce the latent cluster indicators $c_i\in\{1,\dots,K\}$, with $c_i=k$ if $y_i$ belongs to the $k$-th mixture component, then $y_i\mid c_i=k \sim f(y_i\mid \theta^*_k)$, and $Pr(c_i = k \mid \bm{\pi}) = \pi_k$.
%With this notation, and exploiting the conjugacy of the Dirichlet distribution with the multinomial model, we obtain the following full conditionals (where we denote with $\pi(\cdot\mid -)$ the generic full conditional):
%\begin{gather*}
%\pi(\theta^*_k\mid - ) \propto G_0(\theta^*_k) \prod_{i:c_i=k} f(y_i\mid\theta^*_{k})\\
%\pi(\pi_1,\dots,\pi_K\mid -) = \mathrm{Dir}_K(\alpha_1 + n_1,\dots,\alpha_K + n_K) \\
%pr(c_i = k\mid -) = \frac{ \pi_k f(y_i\mid\theta^*_{k}) } {\sum_{k=1}^K \pi_k f(y_i\mid\theta^*_{k}) } 
%\end{gather*}
%where $n_k$ is the number of observations allocated to component $k$, for $k=1,\dots,K$.

\subsection{Dirichlet process mixtures}
Nonparametric mixtures extend model (\ref{eq:ch1_discretemix_h}) by placing a nonparametric prior on $G$. The most common prior on random probability measures is the Dirichlet process (DP), introduced by \citet{ferguson1973, ferguson1974}. Draws from a DP are discrete distributions with probability one, hence they turned out useful as flexible mixing measures in discrete mixtures.

A random distribution $G$ on $\Theta$ is said to follow a DP prior with base measure $G_0$ and concentration parameter $\alpha$, denoted $G\sim\mathrm{DP}(\alpha,G_0)$, if for any partition $\{B_1,\dots,B_H\}$ of $\Theta$, 
\begin{equation*}
(G(B_1),\dots,G(B_H)) \sim \mathrm{Dir}_H\big(\alpha G_0(B_1),\dots,\alpha G_0(B_H)\big).
\end{equation*}
The success of the DP mainly arises from two appealing characteristics: its large support, with respect to the space of probability distributions, and tractability of the posterior distribution.

Closely related to this last aspect is the conjugacy property of the DP: as the finite dimensional Dirichlet distribution is conjugate to the multinomial likelihood, the DP is conjugate with respect to i.i.d. sampling, that is, with respect to a completely unknown distribution from i.i.d. data.
More precisely, if we take $\{\theta_1,\dots,\theta_n\}$ a sequence of independent draws from $G\sim\mathrm{DP}(\alpha,G_0)$, then the posterior distribution of $G$ given these observed values is still a DP. 
Letting again $\{B_1,\dots,B_H\}$ be a finite measurable partition of $\Theta$, and letting $n_h$ be the number of observed values in $B_h$, for $h = 1,\dots,H$, the posterior distribution is given by
\begin{equation*}
(G(B_1),\dots,G(B_H))\mid \theta_1,\dots,\theta_n \sim \mathrm{Dir}_H\big(\alpha G_0(B_1) + n_1,\dots,\alpha G_0(B_H) + n_H\big).
\end{equation*}
In other terms, the posterior distribution is still DP with updated parameters:
\begin{equation*}
G \mid \theta_1,\dots,\theta_n \sim \mathrm{DP} \left(\alpha + n, \frac{\alpha G_0 + \sum_{i=1}^n \delta_{\theta_i}}{\alpha + n} \right).
\end{equation*}

The DP admits several nice representations. An intuitive constructive definition of a DP random probability measure is given by \citet{sethuraman1994} and is based on the discrete nature of the process, which can be represented as a a weighted sum of point masses. This definition states that if  $G\sim \mathrm{DP}(\alpha, G_0)$, then it can be expressed as follows:
\begin{equation*}
\begin{gathered}
\beta_k \sim \mathrm{Beta}(1,\alpha), \qquad \pi_k = \beta_k \prod_{l = 1}^{k-1} (1-\beta_l)\\
\theta^*_k \sim G_0, \qquad
G = \sum_{k=1}^{\infty} \pi_k \delta_{\theta^*_k}.
\end{gathered}
\end{equation*}

From this representation it is straightforward to extend the finite mixture framework described above to nonparametric mixtures: following the structure of equation (\ref{eq:ch1_discretemix_h}), DP mixtures (DPM) can be written as
\begin{equation*}
\begin{gathered}
y_i \mid \theta_i \sim f(y_i\mid\theta_i)\\
\theta_i \mid G \sim G \\
G \sim \mathrm{DP}(\alpha,G_0).
\end{gathered}
\end{equation*}
Differently from the mixtures described in the previous section, DPM are an infinite mixture model, as they assume a countably infinite number of components. 
However, because the $\pi_k$'s decrease exponentially quickly, only a small number of components will be used to model the
data a priori: in the following, we will define \textit{clusters} these non-empty components. In general, the prior expected number of clusters, which corresponds to the number of distinct $\theta_i$ in a sample of $n$ values, is approximately equal to $\alpha \log(1 + n/\alpha)$.
This means that while in finite mixture models the number of clusters has to be fixed in advance, in DP mixture model, the number of clusters is determined by the data and can be inferred.



\subsection{Mixtures of finite mixtures}
To avoid fixing the number of clusters, a different approach to DP mixtures is to consider finite mixtures with a prior on the number of components. Although this may seem as the most natural way to infer the number of groups, application of mixtures of finite mixtures (MFM) has long been hindered by the difficulty of performing posterior inference. 
Several inference methods have been proposed for this type of model \citep{McCullagh2008,nobile2004,nobile2007,richardson1997} often exploiting the reversible jump Markov chain Monte Carlo technique.

More recently, different models have been proposed inspired by nonparametric mixtures. \citet{miller2018} explicitly derive properties of MFM analogous to those exhibited by DPM. They consider a finite mixture model similar to the one defined in equation (\ref{eq:ch1_discretemix_h}), with a prior $p(K)$ on the number of clusters. The limit of their approach is that it is assumed a fixed parameter $\alpha$ for the Dirichlet distribution, regardless of the dimension $K$.

Another approach is discussed by \citet{malsinerwalli2016} and \cite{fs2019}, based on the use of sparse mixtures. Similarly to DPM, in this formulation they distinguish between the mixture components and the clusters, which are defined as the components actually used by the data. The number of components $K$ is fixed to a large and clearly overfitting value, and a Dirichlet prior with small parameter on the mixture weights ensures that the (random) number of clusters $K_+$ will take values smaller than $K$ with high probability a priori and also a posteriori.



\subsection{Bayesian nonparametric models for nested data}
















